{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for both Models for Statistics and Data Processing\n",
    "  compute:\n",
    "  - standard deviation, varianz, mean, median, max, min per residue\n",
    "  - the 10 most pathogenic variants per TMD-JMD region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Libraries and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Project Setup ---\n",
    "from setup_notebook import setup_project_root\n",
    "setup_project_root()\n",
    "\n",
    "# --- Imports ---\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import itertools\n",
    "import pickle\n",
    "from src.project_config import PROJECT_ROOT, RAW_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Original Data Scores\n",
    "pathway_amiss = PROJECT_ROOT / \"data\" / \"raw\" / \"AlphaMissense_csv\"\n",
    "pathway_esm = PROJECT_ROOT / \"data\" / \"processed\" / \"ESM1b_LLR_Normalized_Human_Nout_Proteome\"\n",
    "\n",
    "# Rank scores for both models\n",
    "pathway_amiss_rank = PROJECT_ROOT / \"data\" / \"processed\" / \"AlphaMissense_rank_csv\"\n",
    "pathway_esm_rank = PROJECT_ROOT / \"data\" / \"processed\" / \"ESM1b_rank_csv\"\n",
    "\n",
    "# Dataset with entries\n",
    "pathway_nout_proteome = PROJECT_ROOT / \"results\" / \"csv\" / \"Human_N_Out_Proteome_TMD_pathogenicity_1.csv\"   # UniProt Entries\n",
    "pathway_multispann_proteome = PROJECT_ROOT / \"data\" / \"raw\" / \"multipass.tsv\"                               # UniProt Entries\n",
    "\n",
    "# Load the both dataset with protein UniProt entries\n",
    "human_nout = pd.read_csv(pathway_nout_proteome)\n",
    "multispan_proteome = pd.read_table(pathway_multispann_proteome)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find missing proteins in ESM1b and AlphaMissense Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at /Users/doma/Documents/Bachelor_Arbeit/Code/data/processed/Missing_Entries_N_out_ESM_AM.csv, skipping save.\n"
     ]
    }
   ],
   "source": [
    "# Check missing values in ESM1b and AlphaMissense\n",
    "missing_proteins_esm = []\n",
    "missing_proteins_amiss = []\n",
    "\n",
    "\n",
    "for entry in human_nout['entry']:\n",
    "    # Specify the pathway to each protein\n",
    "    protein_pathway_amiss = os.path.join(pathway_amiss, entry + '.csv')    # Use os.path.isfile(path) returns True if file exists\n",
    "    protein_pathway_esm_rank = os.path.join(pathway_esm_rank, entry + '_LLR_rank.csv')   \n",
    "\n",
    "    # Check AlpaMissense dataset\n",
    "    if not os.path.isfile(protein_pathway_amiss):\n",
    "        missing_proteins_amiss.append(entry)\n",
    "\n",
    "    # Check ESM1b dataset\n",
    "    if not os.path.isfile(protein_pathway_esm_rank):\n",
    "        missing_proteins_esm.append(entry)\n",
    "\n",
    "### special case - Q99102 is too long for alphamissense prediction\n",
    "missing_proteins_amiss.append(\"Q99102\")\n",
    "\n",
    "# Pad the shorter list with None to make both lists the same length\n",
    "max_length = max(len(missing_proteins_esm), len(missing_proteins_amiss))\n",
    "missing_proteins_esm.extend([None] * (max_length - len(missing_proteins_esm)))\n",
    "missing_proteins_amiss.extend([None] * (max_length - len(missing_proteins_amiss)))\n",
    "\n",
    "# Define output path\n",
    "output_path = Path(PROJECT_ROOT) / \"data\" / \"processed\" / \"Missing_Entries_N_out_ESM_AM.csv\"\n",
    "\n",
    "# Check if output file already exists\n",
    "if not output_path.exists():\n",
    "    pd.DataFrame({\n",
    "        'esm1b': missing_proteins_esm,\n",
    "        'amiss': missing_proteins_amiss\n",
    "    }).to_csv(output_path, index=False)\n",
    "    print(f\"File saved to {output_path}\")\n",
    "else:\n",
    "    print(f\"File already exists at {output_path}, skipping save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at /Users/doma/Documents/Bachelor_Arbeit/Code/data/processed/Missing_Entries_Multispan_ESM_AM.csv, skipping save.\n"
     ]
    }
   ],
   "source": [
    "# Check missing values in ESM1b and AlphaMissense in multispan proteome\n",
    "missing_proteins_esm_multispan = []\n",
    "missing_proteins_amiss_multispan = []\n",
    "\n",
    "\n",
    "for entry in multispan_proteome['Entry']:\n",
    "    \n",
    "   # Specify the pathway to each protein\n",
    "    protein_pathway_amiss = os.path.join(pathway_amiss, entry + '.csv')    # Use os.path.isfile(path) returns True if file exists\n",
    "    protein_pathway_esm_rank = os.path.join(pathway_esm_rank, entry + '_LLR_rank.csv')   \n",
    "\n",
    "    # Check AlpaMissense dataset\n",
    "    if not os.path.isfile(protein_pathway_amiss):\n",
    "        missing_proteins_amiss_multispan.append(entry)\n",
    "\n",
    "    # Check ESM1b dataset\n",
    "    if not os.path.isfile(protein_pathway_esm_rank):\n",
    "        missing_proteins_esm_multispan.append(entry)\n",
    "\n",
    "# Pad the shorter list with None to make both lists the same length\n",
    "max_length = max(len(missing_proteins_esm_multispan), len(missing_proteins_amiss_multispan))\n",
    "missing_proteins_esm_multispan.extend([None] * (max_length - len(missing_proteins_esm_multispan)))\n",
    "missing_proteins_amiss_multispan.extend([None] * (max_length - len(missing_proteins_amiss_multispan)))\n",
    "\n",
    "# Define output path\n",
    "output_path = Path(PROJECT_ROOT) / \"data\" / \"processed\" / \"Missing_Entries_Multispan_ESM_AM.csv\"\n",
    "\n",
    "# Check if output file already exists\n",
    "if not output_path.exists():\n",
    "    pd.DataFrame({\n",
    "        'esm1b': missing_proteins_esm_multispan,\n",
    "        'amiss': missing_proteins_amiss_multispan\n",
    "    }).to_csv(output_path, index=False)\n",
    "    print(f\"File saved to {output_path}\")\n",
    "else:\n",
    "    print(f\"File already exists at {output_path}, skipping save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proteins in human N-out that are also in alpha_missense is: 1514\n",
      "Number of proteins in human N-out that are also in esm1b is: 1528\n",
      "Number of proteins in human N-out present in BOTH models: 1514\n",
      "\n",
      "Number of proteins in human Multispan that are also in alpha_missense is: 2827\n",
      "Number of proteins in human Multispan that are also in esm1b is: 2824\n",
      "Number of proteins in human Multispan present in BOTH models: 2822\n"
     ]
    }
   ],
   "source": [
    "# Read CSV files with missing proteins \n",
    "n_out_miss = pd.read_csv(Path(PROJECT_ROOT) / \"data\" / \"processed\" / \"Missing_Entries_N_out_ESM_AM.csv\")\n",
    "multispan_miss = pd.read_csv(Path(PROJECT_ROOT) / \"data\" / \"processed\" / \"Missing_Entries_Multispan_ESM_AM.csv\")\n",
    "\n",
    "# Exclude the missing proteins from further processing in N-out\n",
    "esm_nout_cleaned = human_nout[~human_nout['entry'].isin(n_out_miss[\"esm1b\"])]\n",
    "amiss_nout_cleaned = human_nout[~human_nout['entry'].isin(n_out_miss[\"amiss\"])]\n",
    "\n",
    "# Exclude the missing proteins from further processing in Multispan\n",
    "esm_multispan_cleaned = multispan_proteome[~multispan_proteome['Entry'].isin(multispan_miss[\"esm1b\"])]\n",
    "amiss_multispan_cleaned = multispan_proteome[~multispan_proteome['Entry'].isin(multispan_miss[\"amiss\"])]\n",
    "\n",
    "\n",
    "# Combine missing entries from both sources\n",
    "missing_any = set(n_out_miss[\"esm1b\"]) | set(n_out_miss[\"amiss\"])\n",
    "missing_multispan_any = set(multispan_miss[\"esm1b\"]) | set(multispan_miss[\"amiss\"])\n",
    "\n",
    "# Keep only proteins present in BOTH models\n",
    "proteins_nout_cleaned = human_nout[~human_nout['entry'].isin(missing_any)].copy()\n",
    "proteins_multispan_cleaned = multispan_proteome[~multispan_proteome['Entry'].isin(missing_multispan_any)].copy()\n",
    "\n",
    "\n",
    "print(f\"Number of proteins in human N-out that are also in alpha_missense is: {len(amiss_nout_cleaned)}\")\n",
    "print(f\"Number of proteins in human N-out that are also in esm1b is: {len(esm_nout_cleaned)}\")\n",
    "print(f\"Number of proteins in human N-out present in BOTH models: {len(proteins_nout_cleaned)}\")\n",
    "print(\"\")\n",
    "\n",
    "print(f\"Number of proteins in human Multispan that are also in alpha_missense is: {len(amiss_multispan_cleaned)}\")\n",
    "print(f\"Number of proteins in human Multispan that are also in esm1b is: {len(esm_multispan_cleaned)}\")\n",
    "print(f\"Number of proteins in human Multispan present in BOTH models: {len(proteins_multispan_cleaned)}\")\n",
    "\n",
    "\n",
    "# Define output path\n",
    "output_path_nout_cleaned = Path(PROJECT_ROOT) / \"data\" / \"processed\" / \"N_out_proteome_cleaned.csv\"\n",
    "output_path_multispan_cleaned = Path(PROJECT_ROOT) / \"data\" / \"processed\" / \"Multispan_proteome_cleaned.csv\"\n",
    "\n",
    "proteins_nout_cleaned.to_csv(output_path_nout_cleaned, index=False)\n",
    "proteins_multispan_cleaned.to_csv(output_path_multispan_cleaned, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_get_uniprot_json(uniprot_id, retries=5, delay=3):\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}.json\"\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            return r.json()\n",
    "        except (requests.exceptions.SSLError, requests.exceptions.ConnectionError) as e:\n",
    "            print(f\"[Retry {attempt}/{retries}] SSL error for {uniprot_id}: {e}\")\n",
    "            time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[Request error] {uniprot_id}: {e}\")\n",
    "            break\n",
    "    print(f\"[FAIL] Could not fetch UniProt data for {uniprot_id}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structural_features(uniprot_id, feature_types=None):\n",
    "    if feature_types is None:\n",
    "        return (\"Feature Types: 'Turn', 'Glycosylation', 'Binding site', 'Motif', \"\n",
    "                \"'Sequence conflict', 'Signal', 'Site', 'Modified residue', 'Helix', \"\n",
    "                \"'Natural variant', 'Domain', 'Mutagenesis', 'Cross-link', 'Beta strand', \"\n",
    "                \"'Disulfide bond', 'Alternative sequence', 'Region', 'Compositional bias', \"\n",
    "                \"'Transmembrane', 'Topological domain', 'Peptide', 'Chain'\")\n",
    "\n",
    "    data = safe_get_uniprot_json(uniprot_id)\n",
    "    if data is None:\n",
    "        return [], None  # fail safely\n",
    "\n",
    "    features = data.get('features', [])\n",
    "    feature_list = []\n",
    "\n",
    "    for feature in features:\n",
    "        if feature['type'] in feature_types:\n",
    "            if 'end' in feature['location']:\n",
    "                start = int(feature['location']['start']['value'])\n",
    "                end = int(feature['location']['end']['value'])\n",
    "            else:\n",
    "                start = end = int(feature['location']['start']['value'])\n",
    "            feature_list.append({\n",
    "                'type': feature['type'],\n",
    "                'description': feature.get('description', ''),\n",
    "                'start': start,\n",
    "                'end': end\n",
    "            })\n",
    "\n",
    "    # sequence length with fallback\n",
    "    seq_length = None\n",
    "    seq_info = data.get('sequence')\n",
    "    if seq_info:\n",
    "        if 'length' in seq_info:\n",
    "            try:\n",
    "                seq_length = int(seq_info['length'])\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "        elif 'value' in seq_info:\n",
    "            try:\n",
    "                seq_length = len(seq_info['value'])\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return feature_list, seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying add juxtamembrane positions \n",
    "def map_features_to_residues_multi(\n",
    "        seq_length,\n",
    "        feature_list,\n",
    "        feature_types=None,\n",
    "        jmd_pad=10        # ±-window around each TM span\n",
    "    ):\n",
    "\n",
    "    \"\"\"\n",
    "    Build a per-residue annotation frame.\n",
    "    • data accessed from UniProt\n",
    "\n",
    "    • feature_types are copied 1-to-1 into their own columns  \n",
    "    • a new 'Juxtamembrane' column is filled with '1' for\n",
    "      residues lying jmd_pad residues just outside any TM span\n",
    "    \"\"\"\n",
    "    # Check if feature_types is not None\n",
    "    if feature_types is None:\n",
    "        return ('Feature Types: \"Region\", \"Site\", \"Helix\", \"Beta strand\", \"Transmembrane\", \"Modified residue\", \"Glycosylation\"')\n",
    "\n",
    "    # ➊ allocate empty strings for each feature column\n",
    "    annot_dict = {ft: [''] * seq_length for ft in feature_types}\n",
    "    jmd        = [''] * seq_length      # new column\n",
    "\n",
    "    # ➋ first pass – fill normal feature columns and collect TM ranges\n",
    "    tm_ranges = []                      # (start,end) tuples, 1-based\n",
    "    for feat in feature_list:\n",
    "        ftype = feat['type']\n",
    "        if ftype not in feature_types:\n",
    "            continue\n",
    "        start, end = feat['start'], feat['end']\n",
    "        label      = feat.get('description', '') or '1'\n",
    "\n",
    "        for pos in range(start, end + 1):\n",
    "            if 1 <= pos <= seq_length:\n",
    "                idx = pos - 1\n",
    "                annot_dict[ftype][idx] = (\n",
    "                    f\"{annot_dict[ftype][idx]},{label}\" if annot_dict[ftype][idx] else label\n",
    "                )\n",
    "\n",
    "        if ftype == \"Transmembrane\":\n",
    "            tm_ranges.append((start, end))\n",
    "\n",
    "    # ➌ second pass – flag Juxtamembrane (±10 outside each TM core)\n",
    "    for s, e in tm_ranges:\n",
    "        for pos in range(max(1, s - jmd_pad), s):           # before TM\n",
    "            jmd[pos - 1] = '1'\n",
    "        for pos in range(e + 1, min(seq_length, e + jmd_pad) + 1):  # after TM\n",
    "            jmd[pos - 1] = '1'\n",
    "\n",
    "    # ➍ build the output DataFrame\n",
    "    df = pd.DataFrame({'residue_position': range(1, seq_length + 1)})\n",
    "    for ftype in feature_types:\n",
    "        df[ftype] = annot_dict[ftype]\n",
    "    df['Juxtamembrane'] = jmd          # <- new column\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_top_variants(seq_len, top_df):\n",
    "    annot = [''] * seq_len\n",
    "    for rank, row in enumerate(top_df.itertuples(), 1):\n",
    "        annot[row.residue_position - 1] += f\"{rank}{row.variation},\"\n",
    "    annot = [a.rstrip(',') for a in annot]\n",
    "    return annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_transformation_df(esm1b_path, model=\"am_score\"):\n",
    "    df = pd.read_csv(esm1b_path, index_col=0)\n",
    "\n",
    "    # Clip all values in the DataFrame to [0, 1] before reshaping\n",
    "    df = df.clip(lower=0, upper=1)\n",
    "\n",
    "    # Columns: e.g. 'M 1', 'L 2', ...\n",
    "    columns = [re.match(r'([A-Z])\\s+(\\d+)', c) for c in df.columns]\n",
    "    wt_res = [m.group(1) for m in columns]\n",
    "    positions = [int(m.group(2)) for m in columns]\n",
    "    tidy = []\n",
    "    for col, wt, pos in zip(df.columns, wt_res, positions):\n",
    "        for mut_aa in df.index:\n",
    "            value = df.loc[mut_aa, col]\n",
    "            if pd.isnull(value): continue\n",
    "            tidy.append({\n",
    "                'residue_position': pos,\n",
    "                'residue': wt,\n",
    "                'variation': mut_aa,\n",
    "                model: value\n",
    "            })\n",
    "    tidy_df = pd.DataFrame(tidy)\n",
    "    return tidy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>A0A087X1C5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Entry\n",
       "count         2822\n",
       "unique        2822\n",
       "top     A0A087X1C5\n",
       "freq             1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Available datasets:\n",
    "n_out = pd.read_csv(output_path_nout_cleaned, usecols=[\"entry\"])\n",
    "multispan =  pd.read_csv(output_path_multispan_cleaned, usecols=[\"Entry\"])\n",
    "\n",
    "multispan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b98c710f054e98b13823d0ad7ffb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing proteins:   0%|          | 0/2822 [00:00<?, ?protein/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Can only merge Series or DataFrame objects, a <class 'str'> was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip to next protein\u001b[39;00m\n\u001b[32m     89\u001b[39m df_annot = map_features_to_residues_multi(seq_length_dataset, features)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m merged = \u001b[43mstats\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_annot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mresidue_position\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mleft\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m### Add 2D-Structures from DSSP .pkl data \u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# Load the .pkl dataset \u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(RAW_DIR / \u001b[33m'\u001b[39m\u001b[33m0000DSSP_result_complete_dict.pkl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Bachelor_Arbeit/Code/.venv/lib/python3.12/site-packages/pandas/core/frame.py:10832\u001b[39m, in \u001b[36mDataFrame.merge\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10813\u001b[39m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m  10814\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents=\u001b[32m2\u001b[39m)\n\u001b[32m  10815\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmerge\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m  10828\u001b[39m     validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10829\u001b[39m ) -> DataFrame:\n\u001b[32m  10830\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m> \u001b[39m\u001b[32m10832\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10833\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10834\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10835\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10836\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10837\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10839\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10841\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10842\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10846\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Bachelor_Arbeit/Code/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py:153\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mleft : DataFrame or named Series\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents=\u001b[32m0\u001b[39m)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmerge\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m     validate: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    151\u001b[39m ) -> DataFrame:\n\u001b[32m    152\u001b[39m     left_df = _validate_operand(left)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     right_df = \u001b[43m_validate_operand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m how == \u001b[33m\"\u001b[39m\u001b[33mcross\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[32m    156\u001b[39m             left_df,\n\u001b[32m    157\u001b[39m             right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Bachelor_Arbeit/Code/.venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py:2692\u001b[39m, in \u001b[36m_validate_operand\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m   2690\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.to_frame()\n\u001b[32m   2691\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2692\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   2693\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCan only merge Series or DataFrame objects, a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m was passed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2694\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: Can only merge Series or DataFrame objects, a <class 'str'> was passed"
     ]
    }
   ],
   "source": [
    "# New Stuff to be added (19.7)\n",
    "# 1. protein_statistics CSV files should have now topology - extra/intracellular and transmembrane DONE\n",
    "# 2. Omit Site, Helix, Beta, Modified residue, Glycosylation DONE\n",
    "\n",
    "# Load the .pkl dataset  - 2D structures from DSSP (Tim)\n",
    "with open(RAW_DIR / '0000DSSP_result_complete_dict.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Available datasets:\n",
    "n_out = pd.read_csv(output_path_nout_cleaned, usecols=[\"entry\"])\n",
    "multispan =  pd.read_csv(output_path_multispan_cleaned, usecols=[\"Entry\"])\n",
    "dssp_dataset = data.keys()  \n",
    "\n",
    "\n",
    "store_pathway = PROJECT_ROOT / \"data\" / \"processed\" / \"Multispan_Statistics_Rank\"\n",
    "missaligned = PROJECT_ROOT / \"data\" / \"processed\" / \"missaligned_proteins_multispan_rank.csv\" \n",
    "skipped_proteins = []\n",
    "topology = \"multispan\"   # \"singlespan\", \"multispan\"\n",
    "\n",
    "for protein in tqdm(multispan[\"Entry\"], desc=\"Processing proteins\", unit=\"protein\"):\n",
    "     \n",
    "    # Load the .csv file from protein_pathway_am/esm\n",
    "    protein_pathway_am = os.path.join(pathway_amiss_rank, protein + '_rank.csv')\n",
    "    protein_pathway_esm = os.path.join(pathway_esm_rank, protein + '_LLR_rank.csv')\n",
    "    \n",
    "    # Takes as input matrix-formatted pathogenicity scores and transforms for appropriate format for this pipeline\n",
    "    df_protein_AM = matrix_transformation_df(protein_pathway_am, model=\"am_rank_score\")\n",
    "    df_protein_ESM = matrix_transformation_df(protein_pathway_esm, model=\"esm_rank_score\")\n",
    "    \n",
    "    # Group by position and wild-type residue\n",
    "    grouped_AM = df_protein_AM.groupby(['residue_position', 'residue'])\n",
    "    grouped_ESM = df_protein_ESM.groupby(['residue_position', 'residue'])\n",
    "\n",
    "    # Compute statistics\n",
    "    stats_AM = grouped_AM['am_rank_score'].agg(['mean', 'median', 'std', 'max', 'min']).round(4).add_prefix('AM_')\n",
    "    stats_ESM = grouped_ESM['esm_rank_score'].agg(['mean', 'median', 'std', 'max', 'min']).round(4).add_prefix('ESM_')\n",
    "\n",
    "    # Merge the AM and ESM stats\n",
    "    stats = stats_AM.merge(stats_ESM, left_index=True, right_index=True).reset_index()\n",
    "\n",
    "    # Iterate over groups to find top 5 mutants above threshold\n",
    "    threshold_AM = 0.7      # rank score 0.56625 for threshold 0.564\n",
    "    threshold_ESM = 0.7            # rank score 0.50906 for threshold LLR -7.5\n",
    "\n",
    "    # Ensure both groupings have same keys for consistent loop\n",
    "    common_keys = grouped_AM.groups.keys() & grouped_ESM.groups.keys()\n",
    "\n",
    "    # Prepare dictionaries to store the top pathogenic variants\n",
    "    top_mutants_dict_AM = {}\n",
    "    top_mutants_dict_ESM = {}\n",
    "\n",
    "    for key in common_keys:\n",
    "        group_AM = grouped_AM.get_group(key)\n",
    "        group_ESM = grouped_ESM.get_group(key)\n",
    "\n",
    "        # AM\n",
    "        high_am = group_AM[group_AM['am_rank_score'] > threshold_AM]\n",
    "        mutant_am = ','.join(high_am.sort_values('am_rank_score', ascending=False)['variation'].astype(str).str.upper())\n",
    "        top_mutants_dict_AM[key] = mutant_am\n",
    "\n",
    "        # ESM\n",
    "        high_esm = group_ESM[group_ESM['esm_rank_score'] > threshold_ESM]\n",
    "        mutant_esm = ','.join(high_esm.sort_values('esm_rank_score', ascending=False)['variation'].astype(str).str.upper())\n",
    "        top_mutants_dict_ESM[key] = mutant_esm\n",
    "\n",
    "\n",
    "    stats['key'] = list(zip(stats['residue_position'], stats['residue']))\n",
    "    stats['AM_top_var'] = stats['key'].map(top_mutants_dict_AM).fillna('')\n",
    "    stats['ESM_top_var'] = stats['key'].map(top_mutants_dict_ESM).fillna('')\n",
    "    stats.drop(columns='key', inplace=True)\n",
    "    stats = stats.reset_index()  # optional: flatten index\n",
    "    \n",
    "\n",
    "    ### Additionally add features from uniprot API\n",
    "    # Check lengths from dataset and uniprot\n",
    "    seq_length_dataset = int(stats[\"residue_position\"].max())\n",
    "    features, seq_length_uniprot = get_structural_features(protein, feature_types = [\"Region\", \n",
    "                                                                                     \"Topological domain\", \n",
    "                                                                                     \"Transmembrane\",\n",
    "                                                                                     \"Domain\", \n",
    "                                                                                     \"Signal\"])\n",
    "    \n",
    "    # Check length mismatch\n",
    "    if seq_length_uniprot is not None and seq_length_dataset != seq_length_uniprot:\n",
    "        print(f\"[WARNING] Skipping {protein}: seq length mismatch (AM: {seq_length_dataset}, UniProt: {seq_length_uniprot})\")\n",
    "        skipped_proteins.append((protein, seq_length_dataset, seq_length_uniprot))\n",
    "        continue  # skip to next protein\n",
    "    \n",
    "    df_annot = map_features_to_residues_multi(seq_length_dataset, features)\n",
    "    merged = stats.merge(df_annot, on='residue_position', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### Add 2D-Structures from DSSP .pkl data \n",
    "    # Load the .pkl dataset \n",
    "    with open(RAW_DIR / '0000DSSP_result_complete_dict.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # Add 2D_Structures column\n",
    "    merged['2D_Structures'] = merged['residue_position'].apply(\n",
    "        lambda x: data[protein][x][2] if x in data[protein] else None)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ###################################################################################\n",
    "    # Add top ten pathogenicities in TMD and separately for TMD+JMD regions in this form:\n",
    "    # to residue where the most pathogenic variant is, assign 1H, 2P, 5D, \n",
    "    # Meaning the top 1. mutant is at this residue with variant H \n",
    "  \n",
    "    # Step 1: Boolean mask for TMD\n",
    "    tmd_mask = merged['Transmembrane'].astype(bool)\n",
    "    tmd_positions = set(merged.loc[tmd_mask, 'residue_position'])\n",
    "\n",
    "    # Step 2: Filter variants in TMD for AM and ESM\n",
    "    df_tmd_AM = df_protein_AM[df_protein_AM['residue_position'].isin(tmd_positions)]\n",
    "    df_tmd_ESM = df_protein_ESM[df_protein_ESM['residue_position'].isin(tmd_positions)]\n",
    "\n",
    "    # Step 3: Top 10 TMD variants\n",
    "    N = 10\n",
    "    top_tmd_AM = df_tmd_AM.nlargest(N, 'am_rank_score')\n",
    "    top_tmd_ESM = df_tmd_ESM.nlargest(N, 'esm_rank_score')\n",
    "\n",
    "    # Step 4: Annotate TMD top variants\n",
    "    top_annotation_am_tmd = annotate_top_variants(seq_length_dataset, top_tmd_AM)\n",
    "    annot_dict_am_tmd = {i + 1: val for i, val in enumerate(top_annotation_am_tmd)}\n",
    "    merged['AM_TMD_top10'] = merged['residue_position'].map(annot_dict_am_tmd).fillna('')\n",
    "\n",
    "    top_annotation_esm_tmd = annotate_top_variants(seq_length_dataset, top_tmd_ESM)\n",
    "    annot_dict_esm_tmd = {i + 1: val for i, val in enumerate(top_annotation_esm_tmd)}\n",
    "    merged['ESM_TMD_top10'] = merged['residue_position'].map(annot_dict_esm_tmd).fillna('')\n",
    "\n",
    "    # Step 5: Only do TMD+JMD if singlespan\n",
    "    if topology == \"singlespan\":\n",
    "        # Define Juxtamembrane mask and combined region\n",
    "        jmd_mask = merged['Juxtamembrane'].astype(bool)\n",
    "        tmdjmd_mask = tmd_mask | jmd_mask\n",
    "        tmdjmd_positions = set(merged.loc[tmdjmd_mask, 'residue_position'])\n",
    "\n",
    "        # Filter and rank\n",
    "        df_tmdjmd_AM = df_protein_AM[df_protein_AM['residue_position'].isin(tmdjmd_positions)]\n",
    "        df_tmdjmd_ESM = df_protein_ESM[df_protein_ESM['residue_position'].isin(tmdjmd_positions)]\n",
    "\n",
    "        top_tmdjmd_AM = df_tmdjmd_AM.nlargest(N, 'am_rank_score')\n",
    "        top_tmdjmd_ESM = df_tmdjmd_ESM.nlargest(N, 'esm_rank_score')\n",
    "\n",
    "        # Annotate\n",
    "        top_annotation_am_tmdjmd = annotate_top_variants(seq_length_dataset, top_tmdjmd_AM)\n",
    "        annot_dict_am_tmdjmd = {i + 1: val for i, val in enumerate(top_annotation_am_tmdjmd)}\n",
    "        merged['AM_TMDJMD_top10'] = merged['residue_position'].map(annot_dict_am_tmdjmd).fillna('')\n",
    "\n",
    "        top_annotation_esm_tmdjmd = annotate_top_variants(seq_length_dataset, top_tmdjmd_ESM)\n",
    "        annot_dict_esm_tmdjmd = {i + 1: val for i, val in enumerate(top_annotation_esm_tmdjmd)}\n",
    "        merged['ESM_TMDJMD_top10'] = merged['residue_position'].map(annot_dict_esm_tmdjmd).fillna('')\n",
    "\n",
    "    \n",
    "    elif topology == \"multispan\":\n",
    "        # Remove unrelated columns\n",
    "        merged.drop(columns=['Juxtamembrane', 'AM_TMDJMD_top10', 'ESM_TMDJMD_top10', 'AM_TMD_top10', 'ESM_TMD_top10'], inplace=True, errors='ignore')\n",
    "\n",
    "        # Step 0: Normalize \"Helical; Name=1\" → \"Helical\"\n",
    "        merged['Transmembrane'] = merged['Transmembrane'].replace(\n",
    "            to_replace=r'^Helical(;.*)?$', value='Helical', regex=True)\n",
    "\n",
    "        # Step 1: Relabel contiguous Helical regions\n",
    "        helix_id = 1\n",
    "        new_labels = []\n",
    "        in_helix = False\n",
    "\n",
    "        for val in merged['Transmembrane']:\n",
    "            if val == 'Helical':\n",
    "                if not in_helix:\n",
    "                    label = f'Helical_{helix_id}'\n",
    "                    helix_id += 1\n",
    "                    in_helix = True\n",
    "                new_labels.append(label)\n",
    "            else:\n",
    "                in_helix = False\n",
    "                new_labels.append(val)\n",
    "\n",
    "        merged['Transmembrane'] = new_labels\n",
    "\n",
    "        # Step 2: Prepare annotation arrays (same size as sequence)\n",
    "        am_annot = [''] * seq_length_dataset\n",
    "        esm_annot = [''] * seq_length_dataset\n",
    "\n",
    "        # Step 3: Group by helical regions\n",
    "        helical_labels = sorted(set(lab for lab in new_labels if lab and lab.startswith(\"Helical_\")))\n",
    "        \n",
    "        for helix_label in helical_labels:\n",
    "            helix_mask = merged['Transmembrane'] == helix_label\n",
    "            helix_positions = set(merged.loc[helix_mask, 'residue_position'])\n",
    "\n",
    "            # Filter variants\n",
    "            df_helix_AM = df_protein_AM[df_protein_AM['residue_position'].isin(helix_positions)]\n",
    "            df_helix_ESM = df_protein_ESM[df_protein_ESM['residue_position'].isin(helix_positions)]\n",
    "\n",
    "            top_helix_AM = df_helix_AM.nlargest(N, 'am_rank_score')\n",
    "            top_helix_ESM = df_helix_ESM.nlargest(N, 'esm_rank_score')\n",
    "\n",
    "            # Annotate to main arrays\n",
    "            top_am = annotate_top_variants(seq_length_dataset, top_helix_AM)\n",
    "            top_esm = annotate_top_variants(seq_length_dataset, top_helix_ESM)\n",
    "\n",
    "            # Merge into overall annotations\n",
    "            for i in range(seq_length_dataset):\n",
    "                if top_am[i]:\n",
    "                    am_annot[i] += top_am[i] + ','\n",
    "                if top_esm[i]:\n",
    "                    esm_annot[i] += top_esm[i] + ','\n",
    "\n",
    "        # Clean trailing commas\n",
    "        am_annot = [s.rstrip(',') for s in am_annot]\n",
    "        esm_annot = [s.rstrip(',') for s in esm_annot]\n",
    "\n",
    "        # Assign to merged\n",
    "        merged['AM_Top10_Helix'] = merged['residue_position'].map(lambda x: am_annot[x - 1] if 0 < x <= seq_length_dataset else '')\n",
    "        merged['ESM_Top10_Helix'] = merged['residue_position'].map(lambda x: esm_annot[x - 1] if 0 < x <= seq_length_dataset else '')\n",
    "\n",
    "    ###################################################################################\n",
    "\n",
    "    # save as csv\n",
    "    output_path = os.path.join(store_pathway, f\"{protein}_statistics.csv\")\n",
    "    merged.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "skipped_proteins = pd.DataFrame(skipped_proteins, columns=[\"protein_id\", \"seq_length_AM\", \"seq_length_UniProt\"])\n",
    "skipped_proteins.to_csv(missaligned, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Was ist nur in alphamissense and esm1b und was kommt in beiden vor\n",
    "wwie groß ist die schnittmenge in gesamten protein\n",
    "    - wie groß in TMD, Helical, Domains...\n",
    "  \n",
    "- Die unterschiede bilden von den einzelnen \n",
    "\n",
    "- top5_var\tpathog_var - comma separated DONE\n",
    "\n",
    "\n",
    "- Nimm Varianten die die höchste Differenz zwischen den Modellen haben. \n",
    "\n",
    "- Raw daten übereinander DONE\n",
    "\n",
    "\n",
    "- richtig normalizieren und clippen bei normalizierung  DONE\n",
    "  - die raw scores müssen bereits auch geclippt sein, bevor Überlagerung von raw scores von beiden Models\n",
    "\n",
    "\n",
    "Durchschnittswerte der Pathogenizitäten per Aminosäure: \n",
    "- von Human Proteome  DONE\n",
    "- von Human N-out Proteome  DONE\n",
    "  - Unterschiede? transmembrane hydrophobe aminoacids?\n",
    "- Methionin wird immer über 1500 ???\n",
    "- unterschiede zwischen modellen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting ESM1b to AlphaMissense format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>residue_position</th>\n",
       "      <th>residue</th>\n",
       "      <th>variation</th>\n",
       "      <th>esm1b_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>A</td>\n",
       "      <td>0.50391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>V</td>\n",
       "      <td>0.23940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>L</td>\n",
       "      <td>0.35390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>I</td>\n",
       "      <td>0.58870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>F</td>\n",
       "      <td>0.42180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   residue_position residue variation  esm1b_score\n",
       "0                 1       M         A      0.50391\n",
       "1                 1       M         V      0.23940\n",
       "2                 1       M         L      0.35390\n",
       "3                 1       M         I      0.58870\n",
       "4                 1       M         F      0.42180"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_protein_AM = matrix_transformation_df(protein_pathway_am)\n",
    "df_protein_AM.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

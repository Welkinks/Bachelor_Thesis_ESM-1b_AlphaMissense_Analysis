{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for both Models for Statistics and Data Processing\n",
    "  compute:\n",
    "  - standard deviation, varianz, mean, median, max, min per residue\n",
    "  - the 10 most pathogenic variants per TMD-JMD region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Libraries and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/doma/Documents/Bachelor_Arbeit/Code\")\n",
    "\n",
    "# Set up paths relative to notebook location\n",
    "project_root = Path.cwd() \n",
    "\n",
    "# Original Data Scores\n",
    "pathway_amiss = project_root / \"data\" / \"raw\" / \"AlphaMissense_csv\"\n",
    "pathway_esm = project_root / \"data\" / \"processed\" / \"ESM1b_LLR_Normalized_Human_Nout_Proteome\"\n",
    "\n",
    "# Rank scores for both models\n",
    "pathway_amiss_rank = project_root / \"data\" / \"processed\" / \"AlphaMissense_rank_csv\"\n",
    "pathway_esm_rank = project_root / \"data\" / \"processed\" / \"ESM1b_rank_csv\"\n",
    "\n",
    "# Dataset with entries\n",
    "pathway_nout_proteome = project_root / \"results\" / \"csv\" / \"Human_N_Out_Proteome_TMD_pathogenicity_1.csv\"   # UniProt Entries\n",
    "pathway_multispann_proteome = project_root / \"data\" / \"raw\" / \"multipass.tsv\"                               # UniProt Entries\n",
    "\n",
    "# Load the both dataset with protein UniProt entries\n",
    "human_nout = pd.read_csv(pathway_nout_proteome)\n",
    "multispan_proteome = pd.read_table(pathway_multispann_proteome)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find missing proteins in ESM1b and AlphaMissense Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to /Users/doma/Documents/Bachelor_Arbeit/Code/data/processed/Missing_Entries_N_out_ESM_AM.csv\n"
     ]
    }
   ],
   "source": [
    "# Check missing values in ESM1b and AlphaMissense\n",
    "missing_proteins_esm = []\n",
    "missing_proteins_amiss = []\n",
    "\n",
    "\n",
    "for entry in human_nout['entry']:\n",
    "    # Specify the pathway to each protein\n",
    "    protein_pathway_amiss = os.path.join(pathway_amiss, entry + '.csv')    # Use os.path.isfile(path) returns True if file exists\n",
    "    protein_pathway_esm_rank = os.path.join(pathway_esm_rank, entry + '_LLR_rank.csv')   \n",
    "\n",
    "    # Check AlpaMissense dataset\n",
    "    if not os.path.isfile(protein_pathway_amiss):\n",
    "        missing_proteins_amiss.append(entry)\n",
    "\n",
    "    # Check ESM1b dataset\n",
    "    if not os.path.isfile(protein_pathway_esm_rank):\n",
    "        missing_proteins_esm.append(entry)\n",
    "\n",
    "### special case - Q99102 is too long for alphamissense prediction\n",
    "missing_proteins_amiss.append(\"Q99102\")\n",
    "\n",
    "# Pad the shorter list with None to make both lists the same length\n",
    "max_length = max(len(missing_proteins_esm), len(missing_proteins_amiss))\n",
    "missing_proteins_esm.extend([None] * (max_length - len(missing_proteins_esm)))\n",
    "missing_proteins_amiss.extend([None] * (max_length - len(missing_proteins_amiss)))\n",
    "\n",
    "# Define output path\n",
    "output_path = Path(project_root) / \"data\" / \"processed\" / \"Missing_Entries_N_out_ESM_AM.csv\"\n",
    "\n",
    "# Check if output file already exists\n",
    "if not output_path.exists():\n",
    "    pd.DataFrame({\n",
    "        'esm1b': missing_proteins_esm,\n",
    "        'amiss': missing_proteins_amiss\n",
    "    }).to_csv(output_path, index=False)\n",
    "    print(f\"File saved to {output_path}\")\n",
    "else:\n",
    "    print(f\"File already exists at {output_path}, skipping save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at /Users/doma/Documents/Bachelor_Arbeit/Code/data/processed/Missing_Entries_Multispan_ESM_AM.csv, skipping save.\n"
     ]
    }
   ],
   "source": [
    "# Check missing values in ESM1b and AlphaMissense in multispan proteome\n",
    "missing_proteins_esm_multispan = []\n",
    "missing_proteins_amiss_multispan = []\n",
    "\n",
    "\n",
    "for entry in multispan_proteome['Entry']:\n",
    "    \n",
    "   # Specify the pathway to each protein\n",
    "    protein_pathway_amiss = os.path.join(pathway_amiss, entry + '.csv')    # Use os.path.isfile(path) returns True if file exists\n",
    "    protein_pathway_esm_rank = os.path.join(pathway_esm_rank, entry + '_LLR_rank.csv')   \n",
    "\n",
    "    # Check AlpaMissense dataset\n",
    "    if not os.path.isfile(protein_pathway_amiss):\n",
    "        missing_proteins_amiss_multispan.append(entry)\n",
    "\n",
    "    # Check ESM1b dataset\n",
    "    if not os.path.isfile(protein_pathway_esm_rank):\n",
    "        missing_proteins_esm_multispan.append(entry)\n",
    "\n",
    "# Pad the shorter list with None to make both lists the same length\n",
    "max_length = max(len(missing_proteins_esm_multispan), len(missing_proteins_amiss_multispan))\n",
    "missing_proteins_esm_multispan.extend([None] * (max_length - len(missing_proteins_esm_multispan)))\n",
    "missing_proteins_amiss_multispan.extend([None] * (max_length - len(missing_proteins_amiss_multispan)))\n",
    "\n",
    "# Define output path\n",
    "output_path = Path(project_root) / \"data\" / \"processed\" / \"Missing_Entries_Multispan_ESM_AM.csv\"\n",
    "\n",
    "# Check if output file already exists\n",
    "if not output_path.exists():\n",
    "    pd.DataFrame({\n",
    "        'esm1b': missing_proteins_esm_multispan,\n",
    "        'amiss': missing_proteins_amiss_multispan\n",
    "    }).to_csv(output_path, index=False)\n",
    "    print(f\"File saved to {output_path}\")\n",
    "else:\n",
    "    print(f\"File already exists at {output_path}, skipping save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proteins in human N-out that are also in alpha_missense is: 1514\n",
      "Number of proteins in human N-out that are also in esm1b is: 1528\n",
      "Number of proteins in human N-out present in BOTH models: 1514\n",
      "\n",
      "Number of proteins in human Multispan that are also in alpha_missense is: 2827\n",
      "Number of proteins in human Multispan that are also in esm1b is: 2824\n",
      "Number of proteins in human Multispan present in BOTH models: 2822\n"
     ]
    }
   ],
   "source": [
    "# Read CSV files with missing proteins \n",
    "n_out_miss = pd.read_csv(Path(project_root) / \"data\" / \"processed\" / \"Missing_Entries_N_out_ESM_AM.csv\")\n",
    "multispan_miss = pd.read_csv(Path(project_root) / \"data\" / \"processed\" / \"Missing_Entries_Multispan_ESM_AM.csv\")\n",
    "\n",
    "# Exclude the missing proteins from further processing in N-out\n",
    "esm_nout_cleaned = human_nout[~human_nout['entry'].isin(n_out_miss[\"esm1b\"])]\n",
    "amiss_nout_cleaned = human_nout[~human_nout['entry'].isin(n_out_miss[\"amiss\"])]\n",
    "\n",
    "# Exclude the missing proteins from further processing in Multispan\n",
    "esm_multispan_cleaned = multispan_proteome[~multispan_proteome['Entry'].isin(multispan_miss[\"esm1b\"])]\n",
    "amiss_multispan_cleaned = multispan_proteome[~multispan_proteome['Entry'].isin(multispan_miss[\"amiss\"])]\n",
    "\n",
    "\n",
    "# Combine missing entries from both sources\n",
    "missing_any = set(n_out_miss[\"esm1b\"]) | set(n_out_miss[\"amiss\"])\n",
    "missing_multispan_any = set(multispan_miss[\"esm1b\"]) | set(multispan_miss[\"amiss\"])\n",
    "\n",
    "# Keep only proteins present in BOTH models\n",
    "proteins_nout_cleaned = human_nout[~human_nout['entry'].isin(missing_any)].copy()\n",
    "proteins_multispan_cleaned = multispan_proteome[~multispan_proteome['Entry'].isin(missing_multispan_any)].copy()\n",
    "\n",
    "\n",
    "print(f\"Number of proteins in human N-out that are also in alpha_missense is: {len(amiss_nout_cleaned)}\")\n",
    "print(f\"Number of proteins in human N-out that are also in esm1b is: {len(esm_nout_cleaned)}\")\n",
    "print(f\"Number of proteins in human N-out present in BOTH models: {len(proteins_nout_cleaned)}\")\n",
    "print(\"\")\n",
    "\n",
    "print(f\"Number of proteins in human Multispan that are also in alpha_missense is: {len(amiss_multispan_cleaned)}\")\n",
    "print(f\"Number of proteins in human Multispan that are also in esm1b is: {len(esm_multispan_cleaned)}\")\n",
    "print(f\"Number of proteins in human Multispan present in BOTH models: {len(proteins_multispan_cleaned)}\")\n",
    "\n",
    "\n",
    "# Define output path\n",
    "output_path_nout_cleaned = Path(project_root) / \"data\" / \"processed\" / \"N_out_proteome_cleaned.csv\"\n",
    "output_path_multispan_cleaned = Path(project_root) / \"data\" / \"processed\" / \"Multispan_proteome_cleaned.csv\"\n",
    "\n",
    "proteins_nout_cleaned.to_csv(output_path_nout_cleaned, index=False)\n",
    "proteins_multispan_cleaned.to_csv(output_path_multispan_cleaned, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_get_uniprot_json(uniprot_id, retries=5, delay=3):\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}.json\"\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            return r.json()\n",
    "        except (requests.exceptions.SSLError, requests.exceptions.ConnectionError) as e:\n",
    "            print(f\"[Retry {attempt}/{retries}] SSL error for {uniprot_id}: {e}\")\n",
    "            time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[Request error] {uniprot_id}: {e}\")\n",
    "            break\n",
    "    print(f\"[FAIL] Could not fetch UniProt data for {uniprot_id}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structural_features(uniprot_id, feature_types=None):\n",
    "    if feature_types is None:\n",
    "        feature_types = [\n",
    "            \"Region\", \"Site\", \"Helix\", \"Beta strand\", \n",
    "            \"Transmembrane\", \"Modified residue\", \"Glycosylation\"\n",
    "        ]\n",
    "    else:\n",
    "        return (\"Feature Types: 'Turn', 'Glycosylation', 'Binding site', 'Motif', \"\n",
    "                \"'Sequence conflict', 'Signal', 'Site', 'Modified residue', 'Helix', \"\n",
    "                \"'Natural variant', 'Domain', 'Mutagenesis', 'Cross-link', 'Beta strand', \"\n",
    "                \"'Disulfide bond', 'Alternative sequence', 'Region', 'Compositional bias', \"\n",
    "                \"'Transmembrane', 'Topological domain', 'Peptide', 'Chain'\")\n",
    "\n",
    "    data = safe_get_uniprot_json(uniprot_id)\n",
    "    if data is None:\n",
    "        return [], None  # fail safely\n",
    "\n",
    "    features = data.get('features', [])\n",
    "    feature_list = []\n",
    "\n",
    "    for feature in features:\n",
    "        if feature['type'] in feature_types:\n",
    "            if 'end' in feature['location']:\n",
    "                start = int(feature['location']['start']['value'])\n",
    "                end = int(feature['location']['end']['value'])\n",
    "            else:\n",
    "                start = end = int(feature['location']['start']['value'])\n",
    "            feature_list.append({\n",
    "                'type': feature['type'],\n",
    "                'description': feature.get('description', ''),\n",
    "                'start': start,\n",
    "                'end': end\n",
    "            })\n",
    "\n",
    "    # sequence length with fallback\n",
    "    seq_length = None\n",
    "    seq_info = data.get('sequence')\n",
    "    if seq_info:\n",
    "        if 'length' in seq_info:\n",
    "            try:\n",
    "                seq_length = int(seq_info['length'])\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "        elif 'value' in seq_info:\n",
    "            try:\n",
    "                seq_length = len(seq_info['value'])\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return feature_list, seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying add juxtamembrane positions \n",
    "def map_features_to_residues_multi2(\n",
    "        seq_length,\n",
    "        feature_list,\n",
    "        feature_types=None,\n",
    "        jmd_pad=10        # ±-window around each TM span\n",
    "    ):\n",
    "\n",
    "    \"\"\"\n",
    "    Build a per-residue annotation frame.\n",
    "    • data accessed from UniProt\n",
    "\n",
    "    • feature_types are copied 1-to-1 into their own columns  \n",
    "    • a new 'Juxtamembrane' column is filled with '1' for\n",
    "      residues lying jmd_pad residues just outside any TM span\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if feature_types is None:\n",
    "        feature_types = [\"Region\", \"Site\", \"Helix\", \"Beta strand\",\n",
    "                         \"Transmembrane\", \"Modified residue\", \"Glycosylation\"]\n",
    "\n",
    "    # ➊ allocate empty strings for each feature column\n",
    "    annot_dict = {ft: [''] * seq_length for ft in feature_types}\n",
    "    jmd        = [''] * seq_length      # new column\n",
    "\n",
    "    # ➋ first pass – fill normal feature columns and collect TM ranges\n",
    "    tm_ranges = []                      # (start,end) tuples, 1-based\n",
    "    for feat in feature_list:\n",
    "        ftype = feat['type']\n",
    "        if ftype not in feature_types:\n",
    "            continue\n",
    "        start, end = feat['start'], feat['end']\n",
    "        label      = feat.get('description', '') or '1'\n",
    "\n",
    "        for pos in range(start, end + 1):\n",
    "            if 1 <= pos <= seq_length:\n",
    "                idx = pos - 1\n",
    "                annot_dict[ftype][idx] = (\n",
    "                    f\"{annot_dict[ftype][idx]},{label}\" if annot_dict[ftype][idx] else label\n",
    "                )\n",
    "\n",
    "        if ftype == \"Transmembrane\":\n",
    "            tm_ranges.append((start, end))\n",
    "\n",
    "    # ➌ second pass – flag Juxtamembrane (±10 outside each TM core)\n",
    "    for s, e in tm_ranges:\n",
    "        for pos in range(max(1, s - jmd_pad), s):           # before TM\n",
    "            jmd[pos - 1] = '1'\n",
    "        for pos in range(e + 1, min(seq_length, e + jmd_pad) + 1):  # after TM\n",
    "            jmd[pos - 1] = '1'\n",
    "\n",
    "    # ➍ build the output DataFrame\n",
    "    df = pd.DataFrame({'residue_position': range(1, seq_length + 1)})\n",
    "    for ftype in feature_types:\n",
    "        df[ftype] = annot_dict[ftype]\n",
    "    df['Juxtamembrane'] = jmd          # <- new column\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_top_variants(seq_len, top_df):\n",
    "    annot = [''] * seq_len\n",
    "    for rank, row in enumerate(top_df.itertuples(), 1):\n",
    "        annot[row.residue_position - 1] += f\"{rank}{row.variation},\"\n",
    "    annot = [a.rstrip(',') for a in annot]\n",
    "    return annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_transformation_df(esm1b_path, model=\"am_score\"):\n",
    "    df = pd.read_csv(esm1b_path, index_col=0)\n",
    "\n",
    "    # Clip all values in the DataFrame to [0, 1] before reshaping\n",
    "    df = df.clip(lower=0, upper=1)\n",
    "\n",
    "    # Columns: e.g. 'M 1', 'L 2', ...\n",
    "    columns = [re.match(r'([A-Z])\\s+(\\d+)', c) for c in df.columns]\n",
    "    wt_res = [m.group(1) for m in columns]\n",
    "    positions = [int(m.group(2)) for m in columns]\n",
    "    tidy = []\n",
    "    for col, wt, pos in zip(df.columns, wt_res, positions):\n",
    "        for mut_aa in df.index:\n",
    "            value = df.loc[mut_aa, col]\n",
    "            if pd.isnull(value): continue\n",
    "            tidy.append({\n",
    "                'residue_position': pos,\n",
    "                'residue': wt,\n",
    "                'variation': mut_aa,\n",
    "                model: value\n",
    "            })\n",
    "    tidy_df = pd.DataFrame(tidy)\n",
    "    return tidy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>A0A087X1C5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Entry\n",
       "count         2822\n",
       "unique        2822\n",
       "top     A0A087X1C5\n",
       "freq             1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Available datasets:\n",
    "n_out = pd.read_csv(output_path_nout_cleaned, usecols=[\"entry\"])\n",
    "multispan =  pd.read_csv(output_path_multispan_cleaned, usecols=[\"Entry\"])\n",
    "\n",
    "multispan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68e3fe5eeb342e7be17315171c01e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing proteins:   0%|          | 0/2822 [00:00<?, ?protein/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Skipping P02708: seq length mismatch (AM: 482, UniProt: 457)\n",
      "[WARNING] Skipping P03999: seq length mismatch (AM: 348, UniProt: 345)\n",
      "[WARNING] Skipping P18507: seq length mismatch (AM: 467, UniProt: 475)\n",
      "[WARNING] Skipping P29973: seq length mismatch (AM: 690, UniProt: 686)\n",
      "[WARNING] Skipping P34998: seq length mismatch (AM: 444, UniProt: 415)\n",
      "[WARNING] Skipping Q03518: seq length mismatch (AM: 808, UniProt: 748)\n",
      "[WARNING] Skipping Q0D2K0: seq length mismatch (AM: 466, UniProt: 404)\n",
      "[WARNING] Skipping Q13507: seq length mismatch (AM: 836, UniProt: 921)\n",
      "[WARNING] Skipping Q2M3C6: seq length mismatch (AM: 531, UniProt: 523)\n",
      "[WARNING] Skipping Q3KNS1: seq length mismatch (AM: 767, UniProt: 954)\n",
      "[WARNING] Skipping Q8NBS3: seq length mismatch (AM: 861, UniProt: 875)\n",
      "[WARNING] Skipping Q8TAX9: seq length mismatch (AM: 411, UniProt: 416)\n",
      "[WARNING] Skipping Q8TDS5: seq length mismatch (AM: 423, UniProt: 384)\n",
      "[WARNING] Skipping Q8WUG5: seq length mismatch (AM: 538, UniProt: 649)\n",
      "[WARNING] Skipping Q92581: seq length mismatch (AM: 669, UniProt: 701)\n",
      "[WARNING] Skipping Q93086: seq length mismatch (AM: 422, UniProt: 444)\n",
      "[WARNING] Skipping Q96FT7: seq length mismatch (AM: 647, UniProt: 539)\n",
      "[WARNING] Skipping Q96HE8: seq length mismatch (AM: 216, UniProt: 143)\n",
      "[WARNING] Skipping Q99705: seq length mismatch (AM: 422, UniProt: 353)\n",
      "[WARNING] Skipping Q9BRI3: seq length mismatch (AM: 323, UniProt: 372)\n",
      "[WARNING] Skipping Q9H252: seq length mismatch (AM: 994, UniProt: 958)\n",
      "[WARNING] Skipping Q9H310: seq length mismatch (AM: 441, UniProt: 458)\n",
      "[WARNING] Skipping Q9HBW0: seq length mismatch (AM: 351, UniProt: 348)\n",
      "[WARNING] Skipping Q9Y2W3: seq length mismatch (AM: 782, UniProt: 748)\n",
      "[WARNING] Skipping Q9Y5I7: seq length mismatch (AM: 305, UniProt: 235)\n",
      "[WARNING] Skipping Q8N8F6: seq length mismatch (AM: 280, UniProt: 256)\n",
      "[WARNING] Skipping Q8NGR6: seq length mismatch (AM: 318, UniProt: 317)\n",
      "[WARNING] Skipping Q8TCB6: seq length mismatch (AM: 317, UniProt: 318)\n",
      "[WARNING] Skipping A6NMZ5: seq length mismatch (AM: 311, UniProt: 306)\n",
      "[WARNING] Skipping P0C623: seq length mismatch (AM: 307, UniProt: 314)\n",
      "[WARNING] Skipping P0DN82: seq length mismatch (AM: 309, UniProt: 320)\n",
      "[WARNING] Skipping Q6IFG1: seq length mismatch (AM: 317, UniProt: 313)\n",
      "[WARNING] Skipping Q8NGC6: seq length mismatch (AM: 315, UniProt: 312)\n",
      "[WARNING] Skipping Q8NGI1: seq length mismatch (AM: 322, UniProt: 316)\n",
      "[WARNING] Skipping Q8NGJ2: seq length mismatch (AM: 320, UniProt: 314)\n",
      "[WARNING] Skipping Q8NGR9: seq length mismatch (AM: 330, UniProt: 316)\n",
      "[WARNING] Skipping Q8NGU1: seq length mismatch (AM: 263, UniProt: 314)\n",
      "[WARNING] Skipping Q8NGU4: seq length mismatch (AM: 316, UniProt: 315)\n",
      "[WARNING] Skipping Q8NGV7: seq length mismatch (AM: 314, UniProt: 309)\n",
      "[WARNING] Skipping Q8NH41: seq length mismatch (AM: 348, UniProt: 324)\n",
      "[WARNING] Skipping Q8NHC6: seq length mismatch (AM: 308, UniProt: 325)\n",
      "[WARNING] Skipping Q9P2C4: seq length mismatch (AM: 612, UniProt: 475)\n",
      "[WARNING] Skipping Q3C1V0: seq length mismatch (AM: 400, UniProt: 398)\n",
      "[WARNING] Skipping A0A1B0GVZ9: seq length mismatch (AM: 245, UniProt: 203)\n"
     ]
    }
   ],
   "source": [
    "store_pathway = project_root / \"data\" / \"processed\" / \"Multispan_Statistics_Rank\"\n",
    "missaligned = project_root / \"data\" / \"processed\" / \"missaligned_proteins_multispan_rank.csv\" \n",
    "skipped_proteins = []\n",
    "topology = \"multispan\"   # \"singlespan\", \"multispan\"\n",
    "\n",
    "\n",
    "for protein in tqdm(multispan[\"Entry\"], desc=\"Processing proteins\", unit=\"protein\"):\n",
    "     \n",
    "    # Load the .csv file from protein_pathway_am/esm\n",
    "    protein_pathway_am = os.path.join(pathway_amiss_rank, protein + '_rank.csv')\n",
    "    protein_pathway_esm = os.path.join(pathway_esm_rank, protein + '_LLR_rank.csv')\n",
    "    \n",
    "    # Takes as input matrix-formatted pathogenicity scores and transforms for appropriate format for this pipeline\n",
    "    df_protein_AM = matrix_transformation_df(protein_pathway_am, model=\"am_rank_score\")\n",
    "    df_protein_ESM = matrix_transformation_df(protein_pathway_esm, model=\"esm_rank_score\")\n",
    "    \n",
    "    # Group by position and wild-type residue\n",
    "    grouped_AM = df_protein_AM.groupby(['residue_position', 'residue'])\n",
    "    grouped_ESM = df_protein_ESM.groupby(['residue_position', 'residue'])\n",
    "\n",
    "    # Compute statistics\n",
    "    stats_AM = grouped_AM['am_rank_score'].agg(['mean', 'median', 'std', 'max', 'min']).round(4).add_prefix('AM_')\n",
    "    stats_ESM = grouped_ESM['esm_rank_score'].agg(['mean', 'median', 'std', 'max', 'min']).round(4).add_prefix('ESM_')\n",
    "\n",
    "    # Merge the AM and ESM stats\n",
    "    stats = stats_AM.merge(stats_ESM, left_index=True, right_index=True).reset_index()\n",
    "\n",
    "    # Iterate over groups to find top 5 mutants above threshold\n",
    "    threshold_AM = 0.7      # rank score 0.56625 for threshold 0.564\n",
    "    threshold_ESM = 0.7            # rank score 0.50906 for threshold LLR -7.5\n",
    "\n",
    "    # Ensure both groupings have same keys for consistent loop\n",
    "    common_keys = grouped_AM.groups.keys() & grouped_ESM.groups.keys()\n",
    "\n",
    "    # Prepare dictionaries to store the top pathogenic variants\n",
    "    top_mutants_dict_AM = {}\n",
    "    top_mutants_dict_ESM = {}\n",
    "\n",
    "    for key in common_keys:\n",
    "        group_AM = grouped_AM.get_group(key)\n",
    "        group_ESM = grouped_ESM.get_group(key)\n",
    "\n",
    "        # AM\n",
    "        high_am = group_AM[group_AM['am_rank_score'] > threshold_AM]\n",
    "        mutant_am = ','.join(high_am.sort_values('am_rank_score', ascending=False)['variation'].astype(str).str.upper())\n",
    "        top_mutants_dict_AM[key] = mutant_am\n",
    "\n",
    "        # ESM\n",
    "        high_esm = group_ESM[group_ESM['esm_rank_score'] > threshold_ESM]\n",
    "        mutant_esm = ','.join(high_esm.sort_values('esm_rank_score', ascending=False)['variation'].astype(str).str.upper())\n",
    "        top_mutants_dict_ESM[key] = mutant_esm\n",
    "\n",
    "\n",
    "    stats['key'] = list(zip(stats['residue_position'], stats['residue']))\n",
    "    stats['AM_top_var'] = stats['key'].map(top_mutants_dict_AM).fillna('')\n",
    "    stats['ESM_top_var'] = stats['key'].map(top_mutants_dict_ESM).fillna('')\n",
    "    stats.drop(columns='key', inplace=True)\n",
    "    stats = stats.reset_index()  # optional: flatten index\n",
    "    \n",
    "\n",
    "    ### Additionally add features from uniprot API\n",
    "    # Check lengths from dataset and uniprot\n",
    "    seq_length_dataset = int(stats[\"residue_position\"].max())\n",
    "    features, seq_length_uniprot = get_structural_features(protein)\n",
    "    \n",
    "    # Check length mismatch\n",
    "    if seq_length_uniprot is not None and seq_length_dataset != seq_length_uniprot:\n",
    "        print(f\"[WARNING] Skipping {protein}: seq length mismatch (AM: {seq_length_dataset}, UniProt: {seq_length_uniprot})\")\n",
    "        skipped_proteins.append((protein, seq_length_dataset, seq_length_uniprot))\n",
    "        continue  # skip to next protein\n",
    "    \n",
    "    df_annot = map_features_to_residues_multi2(seq_length_dataset, features)\n",
    "    merged = stats.merge(df_annot, on='residue_position', how='left')\n",
    "\n",
    "\n",
    "    ###################################################################################\n",
    "    # Add top ten pathogenicities in TMD and separately for TMD+JMD regions in this form:\n",
    "    # to residue where the most pathogenic variant is, assign 1H, 2P, 5D, \n",
    "    # Meaning the top 1. mutant is at this residue with variant H \n",
    "        # you already have the transmembrane region. \n",
    "    # Step 1: Boolean mask for TMD\n",
    "    tmd_mask = merged['Transmembrane'].astype(bool)\n",
    "    tmd_positions = set(merged.loc[tmd_mask, 'residue_position'])\n",
    "\n",
    "    # Step 2: Filter variants in TMD for AM and ESM\n",
    "    df_tmd_AM = df_protein_AM[df_protein_AM['residue_position'].isin(tmd_positions)]\n",
    "    df_tmd_ESM = df_protein_ESM[df_protein_ESM['residue_position'].isin(tmd_positions)]\n",
    "\n",
    "    # Step 3: Top 10 TMD variants\n",
    "    N = 10\n",
    "    top_tmd_AM = df_tmd_AM.nlargest(N, 'am_rank_score')\n",
    "    top_tmd_ESM = df_tmd_ESM.nlargest(N, 'esm_rank_score')\n",
    "\n",
    "    # Step 4: Annotate TMD top variants\n",
    "    top_annotation_am_tmd = annotate_top_variants(seq_length_dataset, top_tmd_AM)\n",
    "    annot_dict_am_tmd = {i + 1: val for i, val in enumerate(top_annotation_am_tmd)}\n",
    "    merged['AM_TMD_top10'] = merged['residue_position'].map(annot_dict_am_tmd).fillna('')\n",
    "\n",
    "    top_annotation_esm_tmd = annotate_top_variants(seq_length_dataset, top_tmd_ESM)\n",
    "    annot_dict_esm_tmd = {i + 1: val for i, val in enumerate(top_annotation_esm_tmd)}\n",
    "    merged['ESM_TMD_top10'] = merged['residue_position'].map(annot_dict_esm_tmd).fillna('')\n",
    "\n",
    "    # Step 5: Only do TMD+JMD if singlespan\n",
    "    if topology == \"singlespan\":\n",
    "        # Define Juxtamembrane mask and combined region\n",
    "        jmd_mask = merged['Juxtamembrane'].astype(bool)\n",
    "        tmdjmd_mask = tmd_mask | jmd_mask\n",
    "        tmdjmd_positions = set(merged.loc[tmdjmd_mask, 'residue_position'])\n",
    "\n",
    "        # Filter and rank\n",
    "        df_tmdjmd_AM = df_protein_AM[df_protein_AM['residue_position'].isin(tmdjmd_positions)]\n",
    "        df_tmdjmd_ESM = df_protein_ESM[df_protein_ESM['residue_position'].isin(tmdjmd_positions)]\n",
    "\n",
    "        top_tmdjmd_AM = df_tmdjmd_AM.nlargest(N, 'am_rank_score')\n",
    "        top_tmdjmd_ESM = df_tmdjmd_ESM.nlargest(N, 'esm_rank_score')\n",
    "\n",
    "        # Annotate\n",
    "        top_annotation_am_tmdjmd = annotate_top_variants(seq_length_dataset, top_tmdjmd_AM)\n",
    "        annot_dict_am_tmdjmd = {i + 1: val for i, val in enumerate(top_annotation_am_tmdjmd)}\n",
    "        merged['AM_TMDJMD_top10'] = merged['residue_position'].map(annot_dict_am_tmdjmd).fillna('')\n",
    "\n",
    "        top_annotation_esm_tmdjmd = annotate_top_variants(seq_length_dataset, top_tmdjmd_ESM)\n",
    "        annot_dict_esm_tmdjmd = {i + 1: val for i, val in enumerate(top_annotation_esm_tmdjmd)}\n",
    "        merged['ESM_TMDJMD_top10'] = merged['residue_position'].map(annot_dict_esm_tmdjmd).fillna('')\n",
    "\n",
    "    \n",
    "    elif topology == \"multispan\":\n",
    "        # Remove unrelated columns\n",
    "        merged.drop(columns=['Juxtamembrane', 'AM_TMDJMD_top10', 'ESM_TMDJMD_top10', 'AM_TMD_top10', 'ESM_TMD_top10'], inplace=True, errors='ignore')\n",
    "\n",
    "        # Step 0: Normalize \"Helical; Name=1\" → \"Helical\"\n",
    "        merged['Transmembrane'] = merged['Transmembrane'].replace(\n",
    "            to_replace=r'^Helical(;.*)?$', value='Helical', regex=True)\n",
    "\n",
    "        # Step 1: Relabel contiguous Helical regions\n",
    "        helix_id = 1\n",
    "        new_labels = []\n",
    "        in_helix = False\n",
    "\n",
    "        for val in merged['Transmembrane']:\n",
    "            if val == 'Helical':\n",
    "                if not in_helix:\n",
    "                    label = f'Helical_{helix_id}'\n",
    "                    helix_id += 1\n",
    "                    in_helix = True\n",
    "                new_labels.append(label)\n",
    "            else:\n",
    "                in_helix = False\n",
    "                new_labels.append(val)\n",
    "\n",
    "        merged['Transmembrane'] = new_labels\n",
    "\n",
    "        # Step 2: Prepare annotation arrays (same size as sequence)\n",
    "        am_annot = [''] * seq_length_dataset\n",
    "        esm_annot = [''] * seq_length_dataset\n",
    "\n",
    "        # Step 3: Group by helical regions\n",
    "        helical_labels = sorted(set(lab for lab in new_labels if lab and lab.startswith(\"Helical_\")))\n",
    "        \n",
    "        for helix_label in helical_labels:\n",
    "            helix_mask = merged['Transmembrane'] == helix_label\n",
    "            helix_positions = set(merged.loc[helix_mask, 'residue_position'])\n",
    "\n",
    "            # Filter variants\n",
    "            df_helix_AM = df_protein_AM[df_protein_AM['residue_position'].isin(helix_positions)]\n",
    "            df_helix_ESM = df_protein_ESM[df_protein_ESM['residue_position'].isin(helix_positions)]\n",
    "\n",
    "            top_helix_AM = df_helix_AM.nlargest(N, 'am_rank_score')\n",
    "            top_helix_ESM = df_helix_ESM.nlargest(N, 'esm_rank_score')\n",
    "\n",
    "            # Annotate to main arrays\n",
    "            top_am = annotate_top_variants(seq_length_dataset, top_helix_AM)\n",
    "            top_esm = annotate_top_variants(seq_length_dataset, top_helix_ESM)\n",
    "\n",
    "            # Merge into overall annotations\n",
    "            for i in range(seq_length_dataset):\n",
    "                if top_am[i]:\n",
    "                    am_annot[i] += top_am[i] + ','\n",
    "                if top_esm[i]:\n",
    "                    esm_annot[i] += top_esm[i] + ','\n",
    "\n",
    "        # Clean trailing commas\n",
    "        am_annot = [s.rstrip(',') for s in am_annot]\n",
    "        esm_annot = [s.rstrip(',') for s in esm_annot]\n",
    "\n",
    "        # Assign to merged\n",
    "        merged['AM_Top10_Helix'] = merged['residue_position'].map(lambda x: am_annot[x - 1] if 0 < x <= seq_length_dataset else '')\n",
    "        merged['ESM_Top10_Helix'] = merged['residue_position'].map(lambda x: esm_annot[x - 1] if 0 < x <= seq_length_dataset else '')\n",
    "\n",
    "    ###################################################################################\n",
    "\n",
    "    # save as csv\n",
    "    output_path = os.path.join(store_pathway, f\"{protein}_statistics.csv\")\n",
    "    merged.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "skipped_proteins = pd.DataFrame(skipped_proteins, columns=[\"protein_id\", \"seq_length_AM\", \"seq_length_UniProt\"])\n",
    "skipped_proteins.to_csv(missaligned, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Region', 'description': 'GFLD subdomain', 'start': 28, 'end': 123}\n",
      "{'type': 'Region', 'description': 'CuBD subdomain', 'start': 131, 'end': 189}\n",
      "{'type': 'Region', 'description': 'Disordered', 'start': 194, 'end': 284}\n",
      "{'type': 'Region', 'description': 'Heparin-binding', 'start': 391, 'end': 423}\n",
      "{'type': 'Region', 'description': 'Heparin-binding', 'start': 491, 'end': 522}\n",
      "{'type': 'Region', 'description': 'Collagen-binding', 'start': 523, 'end': 540}\n",
      "{'type': 'Region', 'description': 'Interaction with PSEN1', 'start': 695, 'end': 722}\n",
      "{'type': 'Region', 'description': 'Interaction with G(o)-alpha', 'start': 732, 'end': 751}\n",
      "{'type': 'Region', 'description': 'Required for the interaction with KIF5B and for anterograde transport in axons', 'start': 756, 'end': 770}\n",
      "{'type': 'Site', 'description': 'Required for Cu(2+) reduction', 'start': 170, 'end': 170}\n",
      "{'type': 'Site', 'description': 'Cleavage; by caspases', 'start': 197, 'end': 198}\n",
      "{'type': 'Site', 'description': 'Cleavage; by caspases', 'start': 219, 'end': 220}\n",
      "{'type': 'Site', 'description': 'Reactive bond', 'start': 301, 'end': 302}\n",
      "{'type': 'Site', 'description': 'Cleavage; by beta-secretase', 'start': 671, 'end': 672}\n",
      "{'type': 'Site', 'description': 'Cleavage; by caspase-6; when associated with variant 670-N-L-671', 'start': 672, 'end': 673}\n",
      "{'type': 'Site', 'description': 'Cleavage; by ACE', 'start': 678, 'end': 679}\n",
      "{'type': 'Site', 'description': 'Cleavage; by alpha-secretase', 'start': 687, 'end': 688}\n",
      "{'type': 'Site', 'description': 'Cleavage; by theta-secretase', 'start': 690, 'end': 691}\n",
      "{'type': 'Site', 'description': 'Implicated in free radical propagation', 'start': 704, 'end': 704}\n",
      "{'type': 'Site', 'description': 'Susceptible to oxidation', 'start': 706, 'end': 706}\n",
      "{'type': 'Site', 'description': 'Cleavage; by gamma-secretase; site 1', 'start': 711, 'end': 712}\n",
      "{'type': 'Site', 'description': 'Cleavage; by gamma-secretase; site 2', 'start': 713, 'end': 714}\n",
      "{'type': 'Site', 'description': 'Cleavage; by gamma-secretase; site 3', 'start': 720, 'end': 721}\n",
      "{'type': 'Site', 'description': 'Cleavage; by caspase-6, caspase-8 or caspase-9', 'start': 739, 'end': 740}\n",
      "{'type': 'Helix', 'description': '', 'start': 26, 'end': 28}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 33, 'end': 35}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 43, 'end': 45}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 52, 'end': 54}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 56, 'end': 58}\n",
      "{'type': 'Helix', 'description': '', 'start': 66, 'end': 76}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 82, 'end': 87}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 92, 'end': 94}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 97, 'end': 99}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 103, 'end': 106}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 110, 'end': 112}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 115, 'end': 119}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 134, 'end': 139}\n",
      "{'type': 'Helix', 'description': '', 'start': 147, 'end': 160}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 163, 'end': 174}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 178, 'end': 188}\n",
      "{'type': 'Helix', 'description': '', 'start': 288, 'end': 292}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 299, 'end': 301}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 304, 'end': 310}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 315, 'end': 321}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 323, 'end': 325}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 331, 'end': 333}\n",
      "{'type': 'Helix', 'description': '', 'start': 334, 'end': 341}\n",
      "{'type': 'Helix', 'description': '', 'start': 374, 'end': 380}\n",
      "{'type': 'Helix', 'description': '', 'start': 389, 'end': 418}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 421, 'end': 423}\n",
      "{'type': 'Helix', 'description': '', 'start': 425, 'end': 480}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 482, 'end': 484}\n",
      "{'type': 'Helix', 'description': '', 'start': 487, 'end': 518}\n",
      "{'type': 'Helix', 'description': '', 'start': 520, 'end': 546}\n",
      "{'type': 'Helix', 'description': '', 'start': 547, 'end': 550}\n",
      "{'type': 'Helix', 'description': '', 'start': 552, 'end': 566}\n",
      "{'type': 'Helix', 'description': '', 'start': 615, 'end': 618}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 620, 'end': 622}\n",
      "{'type': 'Helix', 'description': '', 'start': 673, 'end': 675}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 682, 'end': 684}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 688, 'end': 691}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 692, 'end': 694}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 701, 'end': 703}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 707, 'end': 712}\n",
      "{'type': 'Helix', 'description': '', 'start': 713, 'end': 715}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 718, 'end': 720}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 721, 'end': 725}\n",
      "{'type': 'Helix', 'description': '', 'start': 744, 'end': 754}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 756, 'end': 758}\n",
      "{'type': 'Beta strand', 'description': '', 'start': 763, 'end': 765}\n"
     ]
    }
   ],
   "source": [
    "# Define extraction function for getting information from UniProt for further analysis\n",
    "\n",
    "def get_structural_features(uniprot_id, feature_types=None):\n",
    "\n",
    "    if feature_types is None:\n",
    "        feature_types = [\"Region\", \"Site\", \"Helix\", \"Beta strand\", \"Signal\"]\n",
    "    else:\n",
    "        return \"Feature Types: 'Turn', 'Glycosylation', 'Binding site', 'Motif', 'Sequence conflict', 'Signal', 'Site', 'Modified residue', 'Helix', 'Natural variant', 'Domain', 'Mutagenesis', 'Cross-link', 'Beta strand', 'Disulfide bond', 'Alternative sequence', 'Region', 'Compositional bias', 'Transmembrane', 'Topological domain', 'Peptide', 'Chain'\"\n",
    "\n",
    "\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}.json\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    features = data.get('features', [])\n",
    "    feature_list = []\n",
    "\n",
    "\n",
    "    for feature in features:\n",
    "        if feature['type'] in feature_types:\n",
    "            # Handle single-residue vs interval\n",
    "            if 'end' in feature['location']:\n",
    "                start = int(feature['location']['start']['value'])\n",
    "                end = int(feature['location']['end']['value'])\n",
    "            else:\n",
    "                start = end = int(feature['location']['start']['value'])\n",
    "            feature_list.append({\n",
    "                'type': feature['type'],\n",
    "                'description': feature.get('description', ''),\n",
    "                'start': start,\n",
    "                'end': end\n",
    "            })\n",
    "    return feature_list\n",
    "\n",
    "# Example usage:\n",
    "all_features = get_structural_features(\"P05067\")\n",
    "for feat in all_features:\n",
    "    print(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Was ist nur in alphamissense and esm1b und was kommt in beiden vor\n",
    "wwie groß ist die schnittmenge in gesamten protein\n",
    "    - wie groß in TMD, Helical, Domains...\n",
    "  \n",
    "- Die unterschiede bilden von den einzelnen \n",
    "\n",
    "- top5_var\tpathog_var - comma separated DONE\n",
    "\n",
    "\n",
    "- Nimm Varianten die die höchste Differenz zwischen den Modellen haben. \n",
    "\n",
    "- Raw daten übereinander DONE\n",
    "\n",
    "\n",
    "- richtig normalizieren und clippen bei normalizierung  DONE\n",
    "  - die raw scores müssen bereits auch geclippt sein, bevor Überlagerung von raw scores von beiden Models\n",
    "\n",
    "\n",
    "Durchschnittswerte der Pathogenizitäten per Aminosäure: \n",
    "- von Human Proteome  DONE\n",
    "- von Human N-out Proteome  DONE\n",
    "  - Unterschiede? transmembrane hydrophobe aminoacids?\n",
    "- Methionin wird immer über 1500 ???\n",
    "- unterschiede zwischen modellen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting ESM1b to AlphaMissense format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>residue_position</th>\n",
       "      <th>residue</th>\n",
       "      <th>variation</th>\n",
       "      <th>esm1b_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>A</td>\n",
       "      <td>0.50391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>V</td>\n",
       "      <td>0.23940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>L</td>\n",
       "      <td>0.35390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>I</td>\n",
       "      <td>0.58870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>F</td>\n",
       "      <td>0.42180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   residue_position residue variation  esm1b_score\n",
       "0                 1       M         A      0.50391\n",
       "1                 1       M         V      0.23940\n",
       "2                 1       M         L      0.35390\n",
       "3                 1       M         I      0.58870\n",
       "4                 1       M         F      0.42180"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_protein_AM = matrix_transformation_df(protein_pathway_am)\n",
    "df_protein_AM.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
